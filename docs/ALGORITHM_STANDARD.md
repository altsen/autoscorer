# AutoScorer ç®—æ³•å¼€å‘æ ‡å‡†

åŸºäºAutoScorer v2.0è¯„åˆ†ç³»ç»Ÿçš„ç®—æ³•å¼€å‘è§„èŒƒï¼Œæ”¯æŒåŠ¨æ€æ³¨å†Œã€çƒ­é‡è½½å’Œæ¨¡å—åŒ–å¼€å‘ã€‚

## æ¶æ„æ¦‚è§ˆ

AutoScoreré‡‡ç”¨ç°ä»£åŒ–çš„è¯„åˆ†å™¨æ¶æ„ï¼Œæ”¯æŒæ’ä»¶å¼ç®—æ³•å¼€å‘ï¼š

```
src/autoscorer/scorers/
â”œâ”€â”€ __init__.py           # åŒ…åˆå§‹åŒ–
â”œâ”€â”€ registry.py           # åŠ¨æ€æ³¨å†Œå’Œçƒ­é‡è½½ç³»ç»Ÿ
â”œâ”€â”€ base_csv.py          # CSVæ•°æ®å¤„ç†åŸºç±»
â”œâ”€â”€ classification.py    # åˆ†ç±»ä»»åŠ¡è¯„åˆ†å™¨
â”œâ”€â”€ regression.py        # å›å½’ä»»åŠ¡è¯„åˆ†å™¨
â”œâ”€â”€ detection.py         # æ£€æµ‹ä»»åŠ¡è¯„åˆ†å™¨
â””â”€â”€ custom_scorers/      # è‡ªå®šä¹‰è¯„åˆ†å™¨ç›®å½• (æ”¯æŒçƒ­é‡è½½)
    â”œâ”€â”€ example_scorers.py
    â””â”€â”€ hot_reload_test.py
```

## æ ¸å¿ƒç‰¹æ€§

- ğŸ”§ **åŠ¨æ€æ³¨å†Œ**: åŸºäºè£…é¥°å™¨çš„è‡ªåŠ¨æ³¨å†Œæœºåˆ¶
- ğŸ”¥ **çƒ­é‡è½½**: æ–‡ä»¶å˜åŒ–ç›‘æ§å’Œè‡ªåŠ¨é‡æ–°åŠ è½½
- ğŸ“¦ **æ¨¡å—åŒ–**: æ”¯æŒåˆ†ç±»ã€å›å½’ã€æ£€æµ‹ç­‰ä»»åŠ¡ç±»å‹
- ğŸ› ï¸ **æ ‡å‡†åŒ–**: ç»Ÿä¸€çš„æ¥å£å’Œé”™è¯¯å¤„ç†
- ğŸ§ª **æ‰©å±•æ€§**: æ”¯æŒç¬¬ä¸‰æ–¹ç®—æ³•å³æ’å³ç”¨

## è¯„åˆ†å™¨æ³¨å†Œç³»ç»Ÿ

### åŠ¨æ€æ³¨å†Œæœºåˆ¶

AutoScorerä½¿ç”¨åŸºäºè£…é¥°å™¨çš„åŠ¨æ€æ³¨å†Œç³»ç»Ÿï¼Œæ”¯æŒè¿è¡Œæ—¶å‘ç°å’Œç®¡ç†è¯„åˆ†å™¨ï¼š

```python
from autoscorer.scorers.registry import register, ScorerRegistry

# å…¨å±€æ³¨å†Œè¡¨
registry = ScorerRegistry()

# æ³¨å†Œè£…é¥°å™¨
@register("my_scorer")
class MyScorer:
    name = "my_scorer"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # è¯„åˆ†é€»è¾‘å®ç°
        pass
```

### æ³¨å†Œè¡¨åŠŸèƒ½

**æ ¸å¿ƒAPI:**

```python
from autoscorer.scorers.registry import (
    register,           # æ³¨å†Œè£…é¥°å™¨
    get_scorer,         # è·å–è¯„åˆ†å™¨å®ä¾‹
    get_scorer_class,   # è·å–è¯„åˆ†å™¨ç±»
    list_scorers,       # åˆ—å‡ºæ‰€æœ‰è¯„åˆ†å™¨
    load_scorer_file,   # åŠ è½½æ–‡ä»¶ä¸­çš„è¯„åˆ†å™¨
    start_watching_file # å¼€å§‹æ–‡ä»¶ç›‘æ§
)

# ä½¿ç”¨ç¤ºä¾‹
@register("custom_f1")
class CustomF1Scorer:
    name = "custom_f1"
    version = "1.0.0"
    
    def score(self, workspace, params):
        return Result(...)

# è·å–å’Œä½¿ç”¨
scorer = get_scorer("custom_f1")
result = scorer.score(workspace_path, {})
```

### çº¿ç¨‹å®‰å…¨è®¾è®¡

æ³¨å†Œè¡¨é‡‡ç”¨çº¿ç¨‹å®‰å…¨è®¾è®¡ï¼Œæ”¯æŒå¹¶å‘ç¯å¢ƒï¼š

```python
class ScorerRegistry:
    def __init__(self):
        self._scorers = {}
        self._lock = threading.Lock()
    
    def register(self, name: str, scorer_class: type):
        """çº¿ç¨‹å®‰å…¨çš„æ³¨å†Œæ“ä½œ"""
        with self._lock:
            self._scorers[name] = scorer_class
    
    def get(self, name: str):
        """çº¿ç¨‹å®‰å…¨çš„è·å–æ“ä½œ"""
        with self._lock:
            return self._scorers.get(name)
```

## çƒ­é‡è½½ç³»ç»Ÿ

### æ–‡ä»¶ç›‘æ§æœºåˆ¶

AutoScoreræ”¯æŒè‡ªåŠ¨ç›‘æ§æ–‡ä»¶å˜åŒ–å¹¶çƒ­é‡è½½è¯„åˆ†å™¨ï¼š

```python
from autoscorer.scorers.registry import start_watching_file, stop_watching_file

# å¼€å§‹ç›‘æ§æ–‡ä»¶
start_watching_file("custom_scorers/my_scorer.py", check_interval=1.0)

# å½“æ–‡ä»¶ä¿®æ”¹æ—¶ï¼Œç³»ç»Ÿè‡ªåŠ¨é‡æ–°åŠ è½½è¯„åˆ†å™¨
# æ— éœ€é‡å¯æœåŠ¡å³å¯ä½¿ç”¨æ–°çš„è¯„åˆ†å™¨ç‰ˆæœ¬
```

### ç›‘æ§å®ç°åŸç†

```python
def start_watching(self, file_path: str, check_interval: float = 1.0):
    """å¼€å§‹ç›‘æ§æ–‡ä»¶å˜åŒ–å¹¶è‡ªåŠ¨é‡æ–°åŠ è½½"""
    def watch_file():
        last_mtime = 0
        while self._watch_enabled and file_path in self._watchers:
            try:
                path = Path(file_path)
                if path.exists():
                    current_mtime = path.stat().st_mtime
                    if current_mtime > last_mtime:
                        if last_mtime > 0:  # ä¸åœ¨é¦–æ¬¡æ£€æŸ¥æ—¶é‡æ–°åŠ è½½
                            logger.info(f"File changed, reloading: {file_path}")
                            self.reload_file(file_path)
                        last_mtime = current_mtime
                time.sleep(check_interval)
            except Exception as e:
                logger.error(f"Error watching file {file_path}: {e}")
    
    thread = threading.Thread(target=watch_file, daemon=True)
    self._watchers[file_path] = thread
    thread.start()
```

### çƒ­é‡è½½APIä½¿ç”¨

```python
from autoscorer.scorers.registry import (
    load_scorer_file, 
    reload_scorer_file,
    start_watching_file,
    stop_watching_file,
    get_watched_files
)

# 1. åŠ è½½è¯„åˆ†å™¨æ–‡ä»¶
loaded_scorers = load_scorer_file("custom_scorers/my_scorer.py")
print(f"Loaded scorers: {list(loaded_scorers.keys())}")

# 2. å¼€å¯è‡ªåŠ¨ç›‘æ§
start_watching_file("custom_scorers/my_scorer.py")

# 3. æ‰‹åŠ¨é‡æ–°åŠ è½½
reloaded_scorers = reload_scorer_file("custom_scorers/my_scorer.py")

# 4. æŸ¥çœ‹ç›‘æ§çŠ¶æ€
watched_files = get_watched_files()
print(f"Watching files: {watched_files}")

# 5. åœæ­¢ç›‘æ§
stop_watching_file("custom_scorers/my_scorer.py")
```

### Web APIé›†æˆ

çƒ­é‡è½½åŠŸèƒ½å®Œå…¨é›†æˆåˆ°Web APIä¸­ï¼š

```bash
# åˆ—å‡ºæ‰€æœ‰è¯„åˆ†å™¨
curl http://localhost:8000/scorers

# åŠ è½½è¯„åˆ†å™¨æ–‡ä»¶å¹¶å¯ç”¨ç›‘æ§
curl -X POST http://localhost:8000/scorers/load \
  -H 'Content-Type: application/json' \
  -d '{"file_path": "custom_scorers/my_scorer.py", "auto_watch": true}'

# æ‰‹åŠ¨é‡æ–°åŠ è½½
curl -X POST http://localhost:8000/scorers/reload \
  -H 'Content-Type: application/json' \
  -d '{"file_path": "custom_scorers/my_scorer.py"}'

# å¼€å§‹ç›‘æ§æ–‡ä»¶
curl -X POST http://localhost:8000/scorers/watch \
  -H 'Content-Type: application/json' \
  -d '{"file_path": "custom_scorers/my_scorer.py", "check_interval": 1.0}'

# åœæ­¢ç›‘æ§
curl -X DELETE "http://localhost:8000/scorers/watch?file_path=custom_scorers/my_scorer.py"

# æŸ¥çœ‹ç›‘æ§çŠ¶æ€
curl http://localhost:8000/scorers/watch
```

## è¯„åˆ†å™¨å¼€å‘æ ‡å‡†

### æ ¸å¿ƒæ¥å£å®šä¹‰

æ‰€æœ‰è¯„åˆ†å™¨å¿…é¡»åŒ…å«ä»¥ä¸‹å±æ€§å’Œæ–¹æ³•ï¼š

```python
from pathlib import Path
from typing import Dict
from autoscorer.schemas.result import Result

class ScorerInterface:
    name: str          # è¯„åˆ†å™¨å”¯ä¸€æ ‡è¯†ç¬¦
    version: str       # ç‰ˆæœ¬å·
    
    def score(self, workspace: Path, params: Dict) -> Result:
        """
        è¯„åˆ†ä¸»å…¥å£
        Args:
            workspace: å·¥ä½œåŒºè·¯å¾„
            params: è¯„åˆ†å‚æ•°
        Returns:
            Result: æ ‡å‡†åŒ–ç»“æœå¯¹è±¡
        """
        pass
```

### è¯„åˆ†å™¨å®ç°æ¨¡æ¿

```python
from __future__ import annotations
from pathlib import Path
from typing import Dict
from autoscorer.schemas.result import Result
from autoscorer.utils.errors import AutoscorerError
from autoscorer.scorers.registry import register
from autoscorer.scorers.base_csv import BaseCSVScorer

@register("my_algorithm")  # æ³¨å†Œåˆ°ç³»ç»Ÿ
class MyAlgorithm(BaseCSVScorer):  # å¯é€‰ï¼šç»§æ‰¿BaseCSVScorerè·å¾—CSVå¤„ç†èƒ½åŠ›
    name = "my_algorithm"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        """ç®—æ³•è¯„åˆ†å®ç°"""
        try:
            # 1. æ•°æ®åŠ è½½å’ŒéªŒè¯
            gt_data = self._load_ground_truth(workspace)
            pred_data = self._load_predictions(workspace)
            
            # 2. æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
            self._validate_data_consistency(gt_data, pred_data)
            
            # 3. è®¡ç®—æŒ‡æ ‡
            metrics = self._compute_metrics(gt_data, pred_data)
            
            # 4. è¿”å›æ ‡å‡†åŒ–ç»“æœ
            return Result(
                summary={"score": metrics["main_metric"]},
                metrics=metrics,
                artifacts={},
                timing={},
                resources={},
                versioning={
                    "scorer": self.name, 
                    "version": self.version,
                    "timestamp": self._get_iso_timestamp()
                }
            )
            
        except AutoscorerError:
            raise
        except Exception as e:
            raise AutoscorerError(
                code="SCORE_ERROR",
                message=f"Algorithm {self.name} failed: {str(e)}",
                details={"algorithm": self.name, "version": self.version}
            )
    
    def _load_ground_truth(self, workspace: Path):
        """åŠ è½½æ ‡å‡†ç­”æ¡ˆï¼Œç®—æ³•ç‰¹å®šå®ç°"""
        gt_path = workspace / "input" / "gt.csv"
        return self._load_and_validate_csv(gt_path, ["id", "label"])
    
    def _load_predictions(self, workspace: Path):
        """åŠ è½½æ¨¡å‹é¢„æµ‹ï¼Œç®—æ³•ç‰¹å®šå®ç°"""
        pred_path = workspace / "output" / "pred.csv"
        return self._load_and_validate_csv(pred_path, ["id", "label"])
    
    def _validate_data_consistency(self, gt_data, pred_data):
        """æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ"""
        self._validate_id_consistency(gt_data, pred_data)
        # æ·»åŠ ç®—æ³•ç‰¹å®šçš„æ ¡éªŒé€»è¾‘...
    
    def _compute_metrics(self, gt_data, pred_data):
        """è®¡ç®—è¯„åˆ†æŒ‡æ ‡ï¼Œç®—æ³•æ ¸å¿ƒé€»è¾‘"""
        # ç®—æ³•ç‰¹å®šçš„è®¡ç®—é€»è¾‘
        return {"main_metric": 0.85}
```

### çƒ­é‡è½½å…¼å®¹æ€§

ä¸ºäº†æ”¯æŒçƒ­é‡è½½ï¼Œè¯„åˆ†å™¨éœ€è¦éµå¾ªä»¥ä¸‹è§„èŒƒï¼š

```python
# 1. ä½¿ç”¨@registerè£…é¥°å™¨è¿›è¡Œè‡ªåŠ¨æ³¨å†Œ
@register("my_hot_reload_scorer")
class MyHotReloadScorer:
    name = "my_hot_reload_scorer"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # å®ç°ç»†èŠ‚...
        pass

# 2. é¿å…æ¨¡å—çº§å…¨å±€å˜é‡å½±å“é‡åŠ è½½
# é”™è¯¯ç¤ºä¾‹ï¼š
# GLOBAL_CONFIG = {"param": "value"}  # é¿å…ä½¿ç”¨

# æ­£ç¡®ç¤ºä¾‹ï¼š
class MyScorer:
    def __init__(self):
        self.config = {"param": "value"}  # ä½¿ç”¨å®ä¾‹å˜é‡

# 3. æ”¯æŒæ¨¡å—é‡æ–°å¯¼å…¥
if __name__ == "__main__":
    # æµ‹è¯•ä»£ç ï¼Œåœ¨çƒ­é‡è½½æ—¶ä¸ä¼šæ‰§è¡Œ
    pass
```

### è‡ªåŠ¨å‘ç°æœºåˆ¶

ç³»ç»Ÿæ”¯æŒè‡ªåŠ¨å‘ç°æ–‡ä»¶ä¸­çš„è¯„åˆ†å™¨ç±»ï¼š

```python
def load_from_file(self, file_path: str, force_reload: bool = False):
    """ä»æ–‡ä»¶è‡ªåŠ¨å‘ç°å¹¶åŠ è½½è¯„åˆ†å™¨"""
    # å¯¼å…¥æ¨¡å—
    module = self._import_module(file_path)
    
    # è‡ªåŠ¨å‘ç°scorerç±»
    loaded_scorers = {}
    for name, obj in inspect.getmembers(module):
        if (inspect.isclass(obj) and 
            hasattr(obj, 'score') and 
            obj.__module__ == module_name and
            not name.startswith('_')):
            
            # è‡ªåŠ¨æ³¨å†Œ
            scorer_name = getattr(obj, 'name', name.lower())
            self.register(scorer_name, obj)
            loaded_scorers[scorer_name] = obj
    
    return loaded_scorers
```

## æ•°æ®å¤„ç†æ ‡å‡†

### BaseCSVScoreråŸºç±»

å¯¹äºå¤„ç†CSVæ ¼å¼æ•°æ®çš„è¯„åˆ†å™¨ï¼Œæ¨èç»§æ‰¿`BaseCSVScorer`åŸºç±»ï¼š

```python
from autoscorer.scorers.base_csv import BaseCSVScorer

class MyCSVScorer(BaseCSVScorer):
    def _load_ground_truth(self, workspace: Path):
        gt_path = workspace / "input" / "gt.csv"
        return self._load_and_validate_csv(gt_path, ["id", "label"])
    
    def _validate_data_consistency(self, gt_data, pred_data):
        # ä½¿ç”¨åŸºç±»æä¾›çš„IDä¸€è‡´æ€§æ ¡éªŒ
        self._validate_id_consistency(gt_data, pred_data)
        
        # æ·»åŠ ç®—æ³•ç‰¹å®šçš„æ ¡éªŒé€»è¾‘
        for row_id, row in gt_data.items():
            if not self._is_valid_label(row["label"]):
                raise AutoscorerError(
                    code="BAD_FORMAT",
                    message=f"Invalid label format for ID {row_id}: {row['label']}"
                )
```

**BaseCSVScoreræä¾›çš„å·¥å…·æ–¹æ³•:**
- `_load_and_validate_csv(path, required_columns)`: CSVæ–‡ä»¶åŠ è½½å’ŒåŸºç¡€æ ¡éªŒ
- `_validate_id_consistency(gt_data, pred_data)`: IDä¸€è‡´æ€§æ£€æŸ¥
- `_get_iso_timestamp()`: è·å–ISOæ ¼å¼æ—¶é—´æˆ³

### æ•°æ®éªŒè¯è§„èŒƒ

```python
def _validate_data_format(self, gt_data, pred_data):
    """æ ‡å‡†æ•°æ®éªŒè¯æµç¨‹"""
    
    # 1. åŸºç¡€æ ¼å¼æ£€æŸ¥
    if not self._is_valid_format(gt_data):
        raise AutoscorerError(
            code="BAD_FORMAT",
            message="Ground truth format invalid"
        )
    
    # 2. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
    if not self._check_consistency(gt_data, pred_data):
        raise AutoscorerError(
            code="MISMATCH",
            message="Data inconsistency between gt and pred"
        )
    
    # 3. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    if not self._check_completeness(gt_data, pred_data):
        raise AutoscorerError(
            code="MISSING_DATA",
            message="Incomplete data detected"
        )
```

### JSONæ•°æ®å¤„ç†

å¯¹äºJSONæ ¼å¼æ•°æ®ï¼ˆå¦‚æ£€æµ‹ä»»åŠ¡ï¼‰ï¼Œä½¿ç”¨ä»¥ä¸‹æ¨¡å¼ï¼š

```python
def _load_and_validate_json(self, path: Path):
    """JSONæ•°æ®åŠ è½½å’Œæ ¡éªŒ"""
    if not path.exists():
        raise AutoscorerError(code="MISSING_FILE", message=f"File not found: {path}")
    
    try:
        import json
        with path.open('r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise AutoscorerError(
                code="BAD_FORMAT",
                message="JSON must be an array"
            )
        
        return data
        
    except json.JSONDecodeError as e:
        raise AutoscorerError(code="PARSE_ERROR", message=f"Invalid JSON: {e}")
    except UnicodeDecodeError:
        raise AutoscorerError(code="BAD_FORMAT", message="File encoding error, must be UTF-8")
```

## çƒ­é‡è½½å¼€å‘å®è·µ

### æ”¯æŒçƒ­é‡è½½çš„è¯„åˆ†å™¨å¼€å‘

#### 1. åŸºæœ¬å¼€å‘æ¨¡å¼

```python
# custom_scorers/my_hot_scorer.py
from autoscorer.scorers.registry import register
from autoscorer.scorers.base_csv import BaseCSVScorer
from autoscorer.schemas.result import Result

@register("hot_reload_demo")
class HotReloadDemo(BaseCSVScorer):
    """æ”¯æŒçƒ­é‡è½½çš„æ¼”ç¤ºè¯„åˆ†å™¨"""
    
    name = "hot_reload_demo"
    version = "1.0.0"  # ä¿®æ”¹ç‰ˆæœ¬å·æ¥æµ‹è¯•çƒ­é‡è½½
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # è¯„åˆ†é€»è¾‘
        return Result(
            summary={"score": 0.95},  # ä¿®æ”¹è¿™ä¸ªå€¼æ¥æµ‹è¯•çƒ­é‡è½½
            metrics={"accuracy": 0.95},
            versioning={
                "scorer": self.name,
                "version": self.version,
                "message": "Hot reload working!"  # æ·»åŠ æ¶ˆæ¯æ¥éªŒè¯é‡è½½
            }
        )
```

#### 2. å¼€å‘å·¥ä½œæµ

```bash
# ç»ˆç«¯1: å¯åŠ¨APIæœåŠ¡å™¨
autoscorer-api

# ç»ˆç«¯2: åŠ è½½è¯„åˆ†å™¨å¹¶å¯ç”¨ç›‘æ§
curl -X POST http://localhost:8000/scorers/load \
  -H 'Content-Type: application/json' \
  -d '{"file_path": "custom_scorers/my_hot_scorer.py", "auto_watch": true}'

# ç»ˆç«¯3: æµ‹è¯•è¯„åˆ†å™¨
curl -X POST http://localhost:8000/scorers/test \
  -H 'Content-Type: application/json' \
  -d '{"scorer_name": "hot_reload_demo", "workspace": "examples/classification"}'

# ä¿®æ”¹my_hot_scorer.pyæ–‡ä»¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨é‡æ–°åŠ è½½
# å†æ¬¡æµ‹è¯•å¯ä»¥çœ‹åˆ°æ–°çš„ç»“æœ
```

#### 3. çƒ­é‡è½½æœ€ä½³å®è·µ

**é¿å…æ¨¡å—çº§å…¨å±€çŠ¶æ€:**

```python
# âŒ é”™è¯¯æ–¹å¼ - æ¨¡å—çº§å…¨å±€å˜é‡
GLOBAL_CONFIG = {"threshold": 0.5}  # é‡è½½æ—¶å¯èƒ½ä¸ä¼šæ›´æ–°

class BadScorer:
    def score(self, workspace, params):
        threshold = GLOBAL_CONFIG["threshold"]  # å¯èƒ½ä½¿ç”¨æ—§å€¼
        # ...

# âœ… æ­£ç¡®æ–¹å¼ - å®ä¾‹çº§é…ç½®
class GoodScorer:
    def __init__(self):
        self.config = {"threshold": 0.5}  # æ¯æ¬¡å®ä¾‹åŒ–éƒ½ä¼šæ›´æ–°
    
    def score(self, workspace, params):
        threshold = self.config["threshold"]
        # ...
```

**æ”¯æŒå‚æ•°åŒ–é…ç½®:**

```python
@register("configurable_scorer")
class ConfigurableScorer(BaseCSVScorer):
    name = "configurable_scorer"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # ä»paramsè·å–é…ç½®ï¼Œæ”¯æŒåŠ¨æ€å‚æ•°
        threshold = params.get("threshold", 0.5)
        mode = params.get("mode", "standard")
        
        # ä½¿ç”¨é…ç½®è¿›è¡Œè¯„åˆ†
        if mode == "strict":
            threshold *= 1.2
        
        # è¯„åˆ†é€»è¾‘...
        return Result(...)
```

#### 4. ç‰ˆæœ¬ç®¡ç†ç­–ç•¥

```python
@register("versioned_scorer")
class VersionedScorer(BaseCSVScorer):
    name = "versioned_scorer"
    version = "1.2.0"  # è¯­ä¹‰åŒ–ç‰ˆæœ¬å·
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†
        api_version = params.get("api_version", "1.0")
        
        if api_version.startswith("1.0"):
            return self._score_v1_0(workspace, params)
        elif api_version.startswith("1.1"):
            return self._score_v1_1(workspace, params)
        else:
            return self._score_latest(workspace, params)
    
    def _score_v1_0(self, workspace, params):
        """å…¼å®¹v1.0 API"""
        # æ—§ç‰ˆæœ¬é€»è¾‘
        pass
    
    def _score_latest(self, workspace, params):
        """æœ€æ–°ç‰ˆæœ¬é€»è¾‘"""
        # æ–°ç‰ˆæœ¬é€»è¾‘
        pass
```

### ç›‘æ§å’Œè°ƒè¯•

#### æ—¥å¿—é›†æˆ

```python
import logging
from autoscorer.utils.logger import get_logger

@register("logged_scorer")
class LoggedScorer(BaseCSVScorer):
    def __init__(self):
        self.logger = get_logger(f"{__name__}.{self.__class__.__name__}")
    
    def score(self, workspace: Path, params: Dict) -> Result:
        self.logger.info(f"Starting evaluation for workspace: {workspace}")
        
        try:
            # è¯„åˆ†é€»è¾‘
            result = self._compute_score(workspace, params)
            self.logger.info(f"Evaluation completed successfully: {result.summary}")
            return result
            
        except Exception as e:
            self.logger.error(f"Evaluation failed: {e}", exc_info=True)
            raise
```

#### æ€§èƒ½ç›‘æ§

```python
import time
from functools import wraps

def timing_decorator(func):
    """æ€§èƒ½è®¡æ—¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        start_time = time.time()
        try:
            result = func(self, *args, **kwargs)
            execution_time = time.time() - start_time
            
            # æ·»åŠ æ—¶é—´ä¿¡æ¯åˆ°ç»“æœä¸­
            if hasattr(result, 'timing'):
                result.timing.update({
                    f"{func.__name__}_time": execution_time,
                    "total_time": execution_time
                })
            
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.error(f"{func.__name__} failed after {execution_time:.2f}s: {e}")
            raise
    return wrapper

@register("monitored_scorer")
class MonitoredScorer(BaseCSVScorer):
    @timing_decorator
    def score(self, workspace: Path, params: Dict) -> Result:
        # è¯„åˆ†é€»è¾‘
        pass
```

### æµ‹è¯•å’ŒéªŒè¯

#### å•å…ƒæµ‹è¯•æ”¯æŒ

```python
# tests/test_hot_reload_scorer.py
import unittest
import tempfile
from pathlib import Path
from autoscorer.scorers.registry import get_scorer, load_scorer_file

class TestHotReloadScorer(unittest.TestCase):
    def setUp(self):
        """æµ‹è¯•è®¾ç½®"""
        self.temp_dir = tempfile.mkdtemp()
        self.workspace = Path(self.temp_dir)
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        self._create_test_data()
    
    def test_scorer_functionality(self):
        """æµ‹è¯•è¯„åˆ†å™¨åŸºæœ¬åŠŸèƒ½"""
        # åŠ è½½è¯„åˆ†å™¨
        load_scorer_file("custom_scorers/my_hot_scorer.py")
        scorer = get_scorer("hot_reload_demo")
        
        # æ‰§è¡Œè¯„åˆ†
        result = scorer.score(self.workspace, {})
        
        # éªŒè¯ç»“æœ
        self.assertIn("score", result.summary)
        self.assertGreater(result.summary["score"], 0)
    
    def test_hot_reload(self):
        """æµ‹è¯•çƒ­é‡è½½åŠŸèƒ½"""
        # é¦–æ¬¡åŠ è½½
        load_scorer_file("custom_scorers/my_hot_scorer.py")
        scorer1 = get_scorer("hot_reload_demo")
        result1 = scorer1.score(self.workspace, {})
        
        # æ¨¡æ‹Ÿæ–‡ä»¶ä¿®æ”¹å’Œé‡æ–°åŠ è½½
        # (åœ¨å®é™…æµ‹è¯•ä¸­ï¼Œå¯ä»¥ä¿®æ”¹æ–‡ä»¶å†…å®¹)
        load_scorer_file("custom_scorers/my_hot_scorer.py", force_reload=True)
        scorer2 = get_scorer("hot_reload_demo")
        result2 = scorer2.score(self.workspace, {})
        
        # éªŒè¯é‡è½½æˆåŠŸ
        self.assertIsNotNone(result2)
    
    def _create_test_data(self):
        """åˆ›å»ºæµ‹è¯•æ•°æ®"""
        # åˆ›å»ºgt.csv
        gt_path = self.workspace / "input"
        gt_path.mkdir(parents=True, exist_ok=True)
        with (gt_path / "gt.csv").open("w") as f:
            f.write("id,label\n1,cat\n2,dog\n")
        
        # åˆ›å»ºpred.csv
        pred_path = self.workspace / "output"
        pred_path.mkdir(parents=True, exist_ok=True)
        with (pred_path / "pred.csv").open("w") as f:
            f.write("id,label\n1,cat\n2,dog\n")
```

#### é›†æˆæµ‹è¯•

```python
# integration_test.py
import requests
import time

def test_api_hot_reload():
    """æµ‹è¯•APIå±‚é¢çš„çƒ­é‡è½½åŠŸèƒ½"""
    base_url = "http://localhost:8000"
    
    # 1. åŠ è½½è¯„åˆ†å™¨
    response = requests.post(f"{base_url}/scorers/load", json={
        "file_path": "custom_scorers/my_hot_scorer.py",
        "auto_watch": True
    })
    assert response.status_code == 200
    
    # 2. æµ‹è¯•è¯„åˆ†å™¨
    response = requests.post(f"{base_url}/scorers/test", json={
        "scorer_name": "hot_reload_demo",
        "workspace": "examples/classification"
    })
    assert response.status_code == 200
    original_result = response.json()
    
    # 3. æ‰‹åŠ¨é‡æ–°åŠ è½½
    response = requests.post(f"{base_url}/scorers/reload", json={
        "file_path": "custom_scorers/my_hot_scorer.py"
    })
    assert response.status_code == 200
    
    # 4. å†æ¬¡æµ‹è¯•
    response = requests.post(f"{base_url}/scorers/test", json={
        "scorer_name": "hot_reload_demo", 
        "workspace": "examples/classification"
    })
    assert response.status_code == 200
    reloaded_result = response.json()
    
    print("Hot reload test passed!")
    print(f"Original: {original_result['data']['result']['summary']}")
    print(f"Reloaded: {reloaded_result['data']['result']['summary']}")

if __name__ == "__main__":
    test_api_hot_reload()
```

## æ ‡å‡†åŒ–è¾“å‡ºè§„èŒƒ

### Resultå¯¹è±¡ç»“æ„

æ‰€æœ‰è¯„åˆ†å™¨å¿…é¡»è¿”å›æ ‡å‡†åŒ–çš„`Result`å¯¹è±¡ï¼š

```python
from autoscorer.schemas.result import Result

result = Result(
    summary={                    # æ ¸å¿ƒæŒ‡æ ‡æ‘˜è¦
        "score": 0.85,           # ä¸»è¦è¯„åˆ†æŒ‡æ ‡ (å¿…éœ€)
        "rank": "A",             # ç­‰çº§è¯„å®š (å¯é€‰)
        "pass": True             # æ˜¯å¦é€šè¿‡ (å¯é€‰)
    },
    metrics={                    # è¯¦ç»†æŒ‡æ ‡
        "f1_macro": 0.85,
        "precision": 0.83,
        "recall": 0.87,
        "per_class_metrics": {...}
    },
    artifacts={                  # äº§ç‰©æ–‡ä»¶ä¿¡æ¯ (å¯é€‰)
        "confusion_matrix": {
            "path": "/path/to/cm.png",
            "size": 1024,
            "type": "image/png"
        }
    },
    timing={                     # æ€§èƒ½ç»Ÿè®¡ (å¯é€‰)
        "data_loading": 1.2,
        "computation": 5.8,
        "total": 7.0
    },
    resources={                  # èµ„æºä½¿ç”¨ (å¯é€‰)
        "memory_peak": "512MB",
        "cpu_usage": 2.1
    },
    versioning={                 # ç‰ˆæœ¬ä¿¡æ¯ (å¿…éœ€)
        "scorer": "my_algorithm",
        "version": "1.0.0",
        "algorithm": "Custom Algorithm",
        "timestamp": "2024-12-01T10:00:00Z"
    }
)
```

### é”™è¯¯å¤„ç†æ ‡å‡†

```python
from autoscorer.utils.errors import AutoscorerError

# æ ‡å‡†é”™è¯¯ç 
ERROR_CODES = {
    "BAD_FORMAT": "æ•°æ®æ ¼å¼é”™è¯¯",
    "MISMATCH": "æ•°æ®ä¸åŒ¹é…", 
    "MISSING_FILE": "æ–‡ä»¶ç¼ºå¤±",
    "TYPE_ERROR": "æ•°æ®ç±»å‹é”™è¯¯",
    "PARSE_ERROR": "è§£æé”™è¯¯",
    "SCORE_ERROR": "è¯„åˆ†è®¡ç®—é”™è¯¯"
}

# ç»Ÿä¸€é”™è¯¯å¤„ç†
try:
    # è¯„åˆ†é€»è¾‘
    pass
except AutoscorerError:
    raise  # é‡æ–°æŠ›å‡ºAutoscorerError
except Exception as e:
    raise AutoscorerError(
        code="SCORE_ERROR",
        message=f"Unexpected error in {self.name}: {str(e)}",
        details={
            "algorithm": self.name,
            "version": self.version,
            "original_error": str(e)
        }
    )
```

## å®é™…ç®—æ³•å®ç°ç¤ºä¾‹

### 1. åˆ†ç±»F1è¯„åˆ†å™¨ (çƒ­é‡è½½å…¼å®¹)

```python
# custom_scorers/classification_f1_v2.py
from autoscorer.scorers.registry import register
from autoscorer.scorers.base_csv import BaseCSVScorer
from autoscorer.schemas.result import Result
from autoscorer.utils.errors import AutoscorerError

@register("classification_f1_v2")
class ClassificationF1V2(BaseCSVScorer):
    """åˆ†ç±»F1è¯„åˆ†å™¨ - æ”¯æŒçƒ­é‡è½½ç‰ˆæœ¬"""
    
    name = "classification_f1_v2"
    version = "2.1.0"  # ä¿®æ”¹ç‰ˆæœ¬å·æµ‹è¯•çƒ­é‡è½½
    
    def score(self, workspace: Path, params: Dict) -> Result:
        """F1åˆ†æ•°è®¡ç®—ä¸»å…¥å£"""
        try:
            # 1. åŠ è½½å’Œæ ¡éªŒæ•°æ®
            gt_data = self._load_ground_truth(workspace)
            pred_data = self._load_predictions(workspace)
            
            # 2. æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
            self._validate_data_consistency(gt_data, pred_data)
            
            # 3. è®¡ç®—F1æŒ‡æ ‡
            metrics = self._compute_f1_metrics(gt_data, pred_data, params)
            
            # 4. ç”Ÿæˆç­‰çº§å’Œé€šè¿‡çŠ¶æ€
            f1_score = metrics["f1_macro"]
            rank = self._get_rank(f1_score)
            is_pass = f1_score >= params.get("pass_threshold", 0.6)
            
            # 5. è¿”å›æ ‡å‡†åŒ–ç»“æœ
            return Result(
                summary={
                    "score": f1_score,
                    "f1_macro": f1_score,
                    "rank": rank,
                    "pass": is_pass
                },
                metrics=metrics,
                artifacts={},
                timing={},
                resources={},
                versioning={
                    "scorer": self.name,
                    "version": self.version,
                    "algorithm": "Macro-F1 Score with Hot Reload",
                    "timestamp": self._get_iso_timestamp()
                }
            )
            
        except AutoscorerError:
            raise
        except Exception as e:
            raise AutoscorerError(
                code="SCORE_ERROR",
                message=f"F1 calculation failed: {str(e)}",
                details={"algorithm": self.name, "version": self.version}
            )
    
    def _get_rank(self, f1_score: float) -> str:
        """æ ¹æ®F1åˆ†æ•°è®¡ç®—ç­‰çº§"""
        if f1_score >= 0.9:
            return "A+"
        elif f1_score >= 0.8:
            return "A"
        elif f1_score >= 0.7:
            return "B"
        elif f1_score >= 0.6:
            return "C"
        else:
            return "D"
    
    def _compute_f1_metrics(self, gt_data, pred_data, params):
        """è®¡ç®—F1æŒ‡æ ‡ - æ”¯æŒå‚æ•°åŒ–é…ç½®"""
        # è·å–é…ç½®å‚æ•°
        average_method = params.get("average", "macro")  # macro/micro/weighted
        
        # æå–æ ‡ç­¾
        gt_labels = {k: v["label"] for k, v in gt_data.items()}
        pred_labels = {k: v["label"] for k, v in pred_data.items()}
        
        # è·å–æ‰€æœ‰å”¯ä¸€æ ‡ç­¾
        unique_labels = sorted(set(gt_labels.values()))
        
        # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„æŒ‡æ ‡
        per_label_metrics = {}
        total_tp, total_fp, total_fn = 0, 0, 0
        
        for label in unique_labels:
            tp = sum(1 for k in gt_labels 
                    if gt_labels[k] == label and pred_labels[k] == label)
            fp = sum(1 for k in pred_labels 
                    if pred_labels[k] == label and gt_labels[k] != label)
            fn = sum(1 for k in gt_labels 
                    if gt_labels[k] == label and pred_labels[k] != label)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            per_label_metrics[f"f1_{label}"] = f1
            per_label_metrics[f"precision_{label}"] = precision
            per_label_metrics[f"recall_{label}"] = recall
            
            total_tp += tp
            total_fp += fp
            total_fn += fn
        
        # è®¡ç®—ä¸åŒå¹³å‡æ–¹æ³•çš„F1
        if average_method == "macro":
            macro_f1 = sum(per_label_metrics[f"f1_{label}"] for label in unique_labels) / len(unique_labels)
        elif average_method == "micro":
            micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
            micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
            macro_f1 = 2 * micro_precision * micro_recall / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
        
        return {
            "f1_macro": macro_f1,
            "num_labels": len(unique_labels),
            "total_samples": len(gt_labels),
            "average_method": average_method,
            **per_label_metrics
        }
```

### 2. å›å½’è¯„åˆ†å™¨ (æ”¯æŒå¤šæŒ‡æ ‡)

```python
# custom_scorers/regression_multi.py
import math
from autoscorer.scorers.registry import register
from autoscorer.scorers.base_csv import BaseCSVScorer

@register("regression_multi")
class RegressionMultiMetric(BaseCSVScorer):
    """å¤šæŒ‡æ ‡å›å½’è¯„åˆ†å™¨"""
    
    name = "regression_multi"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        try:
            # æ•°æ®åŠ è½½
            gt_data = self._load_ground_truth(workspace)
            pred_data = self._load_predictions(workspace)
            self._validate_data_consistency(gt_data, pred_data)
            
            # è½¬æ¢ä¸ºæ•°å€¼
            gt_values = self._extract_numeric_values(gt_data, "GT")
            pred_values = self._extract_numeric_values(pred_data, "Prediction")
            
            # è®¡ç®—å¤šç§æŒ‡æ ‡
            metrics = self._compute_regression_metrics(gt_values, pred_values, params)
            
            # ä¸»è¦æŒ‡æ ‡é€‰æ‹©
            primary_metric = params.get("primary_metric", "rmse")
            primary_score = metrics[primary_metric]
            
            return Result(
                summary={
                    "score": primary_score,
                    primary_metric: primary_score,
                    "rank": self._get_regression_rank(primary_score, primary_metric)
                },
                metrics=metrics,
                versioning={
                    "scorer": self.name,
                    "version": self.version,
                    "algorithm": f"Multi-Metric Regression ({primary_metric.upper()})",
                    "timestamp": self._get_iso_timestamp()
                }
            )
            
        except Exception as e:
            raise AutoscorerError(
                code="SCORE_ERROR",
                message=f"Regression evaluation failed: {str(e)}"
            )
    
    def _compute_regression_metrics(self, gt_values, pred_values, params):
        """è®¡ç®—å¤šç§å›å½’æŒ‡æ ‡"""
        n = len(gt_values)
        
        # åŸºç¡€è¯¯å·®è®¡ç®—
        errors = [gt - pred for gt, pred in zip(gt_values, pred_values)]
        abs_errors = [abs(e) for e in errors]
        squared_errors = [e ** 2 for e in errors]
        
        # MAE (å¹³å‡ç»å¯¹è¯¯å·®)
        mae = sum(abs_errors) / n
        
        # MSE (å‡æ–¹è¯¯å·®)
        mse = sum(squared_errors) / n
        
        # RMSE (å‡æ–¹æ ¹è¯¯å·®)
        rmse = math.sqrt(mse)
        
        # RÂ² (å†³å®šç³»æ•°)
        gt_mean = sum(gt_values) / n
        ss_tot = sum((gt - gt_mean) ** 2 for gt in gt_values)
        ss_res = sum(squared_errors)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0.0
        
        # MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®)
        mape = sum(abs(e / gt) for e, gt in zip(errors, gt_values) if gt != 0) / n * 100
        
        return {
            "mae": mae,
            "mse": mse,
            "rmse": rmse,
            "r_squared": r_squared,
            "mape": mape,
            "gt_mean": sum(gt_values) / n,
            "pred_mean": sum(pred_values) / n,
            "n_samples": n
        }
```

## éƒ¨ç½²å’Œé›†æˆ

### æ¨¡å—åŒ–éƒ¨ç½²

æ¯ä¸ªè¯„åˆ†å™¨æ¨¡å—å¯ä»¥ç‹¬ç«‹éƒ¨ç½²å’Œçƒ­æ›´æ–°ï¼š

```python
# éƒ¨ç½²æ–°çš„è¯„åˆ†å™¨æ¨¡å—
# custom_scorers/production_scorer.py
from autoscorer.scorers.registry import register
from autoscorer.scorers.base_csv import BaseCSVScorer

@register("production_scorer_v1")
class ProductionScorerV1(BaseCSVScorer):
    name = "production_scorer_v1"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # ç”Ÿäº§ç¯å¢ƒè¯„åˆ†é€»è¾‘
        pass
```

### ç¬¬ä¸‰æ–¹ç®—æ³•é›†æˆ

æ”¯æŒç¬¬ä¸‰æ–¹å¼€å‘è€…åˆ›å»ºç‹¬ç«‹çš„ç®—æ³•åŒ…ï¼š

```python
# third_party_scorers.py
from autoscorer.scorers.registry import register

@register("custom_nlp_scorer")
class CustomNLPScorer:
    name = "custom_nlp_scorer"
    version = "1.0.0"
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # ç¬¬ä¸‰æ–¹NLPè¯„åˆ†å®ç°
        pass

# åŠ¨æ€åŠ è½½ç¬¬ä¸‰æ–¹è¯„åˆ†å™¨
from autoscorer.scorers.registry import load_scorer_file

# åŠ è½½å¹¶å¯ç”¨ç›‘æ§
loaded_scorers = load_scorer_file("third_party_scorers.py")
start_watching_file("third_party_scorers.py")
```

### é…ç½®å’Œç®¡ç†

```python
# è·å–æ‰€æœ‰å·²æ³¨å†Œçš„è¯„åˆ†å™¨
from autoscorer.scorers.registry import list_scorers, get_scorer

# åˆ—å‡ºæ‰€æœ‰è¯„åˆ†å™¨
scorers = list_scorers()
for name, description in scorers.items():
    print(f"è¯„åˆ†å™¨: {name} - {description}")

# åŠ¨æ€è·å–å’Œä½¿ç”¨è¯„åˆ†å™¨
scorer = get_scorer("classification_f1_v2")
if scorer:
    result = scorer.score(workspace, params)
else:
    print("è¯„åˆ†å™¨æœªæ‰¾åˆ°")
```

### ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

#### 1. ç‰ˆæœ¬ç®¡ç†

```python
@register("stable_scorer")
class StableScorer(BaseCSVScorer):
    name = "stable_scorer"
    version = "2.1.0"
    
    # æ”¯æŒAPIç‰ˆæœ¬å…¼å®¹æ€§
    SUPPORTED_API_VERSIONS = ["2.0", "2.1"]
    
    def score(self, workspace: Path, params: Dict) -> Result:
        api_version = params.get("api_version", "2.1")
        
        if api_version not in self.SUPPORTED_API_VERSIONS:
            raise AutoscorerError(
                code="UNSUPPORTED_VERSION",
                message=f"API version {api_version} not supported"
            )
        
        # æ ¹æ®APIç‰ˆæœ¬è°ƒç”¨å¯¹åº”çš„å®ç°
        if api_version.startswith("2.0"):
            return self._score_v2_0(workspace, params)
        else:
            return self._score_v2_1(workspace, params)
```

#### 2. æ€§èƒ½ä¼˜åŒ–

```python
@register("optimized_scorer")
class OptimizedScorer(BaseCSVScorer):
    def __init__(self):
        # ç¼“å­˜æœºåˆ¶
        self._cache = {}
        self._cache_size_limit = 1000
    
    def score(self, workspace: Path, params: Dict) -> Result:
        # ç¼“å­˜é”®
        cache_key = self._generate_cache_key(workspace, params)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self._cache:
            return self._cache[cache_key]
        
        # è®¡ç®—ç»“æœ
        result = self._compute_score(workspace, params)
        
        # ç¼“å­˜ç»“æœ
        if len(self._cache) < self._cache_size_limit:
            self._cache[cache_key] = result
        
        return result
```

#### 3. ç›‘æ§å’Œæ—¥å¿—

```python
@register("monitored_scorer")
class MonitoredScorer(BaseCSVScorer):
    def __init__(self):
        self.metrics_collector = MetricsCollector()
    
    def score(self, workspace: Path, params: Dict) -> Result:
        start_time = time.time()
        
        try:
            result = self._compute_score(workspace, params)
            
            # è®°å½•æˆåŠŸæŒ‡æ ‡
            self.metrics_collector.record_success(
                scorer=self.name,
                execution_time=time.time() - start_time,
                score=result.summary.get("score")
            )
            
            return result
            
        except Exception as e:
            # è®°å½•å¤±è´¥æŒ‡æ ‡
            self.metrics_collector.record_failure(
                scorer=self.name,
                error_type=type(e).__name__,
                execution_time=time.time() - start_time
            )
            raise
```
